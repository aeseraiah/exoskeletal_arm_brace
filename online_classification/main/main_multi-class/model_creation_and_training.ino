aimodel_t model;                                    // AIfES model
ailayer_t *x;                                       // Layer object from AIfES, contains the layers

void *parameter_memory = NULL;                      // Pointer to the memory stack of the AIfES model

const int num_classes = 3;
const int num_features = 2;

uint16_t input_shape[] = {1, num_features};   

ailayer_input_f32_t   input_layer     = AILAYER_INPUT_F32_A( /*input dimension=*/ num_features, /*input shape=*/ input_shape); 
ailayer_dense_f32_t   dense_layer_1   = AILAYER_DENSE_F32_A( /*neurons=*/ 60); 

ailayer_sigmoid_f32_t sigmoid_layer_1 = AILAYER_SIGMOID_F32_A(); 

// This part is for the sigmoid activation function on the output layer and the mean squared error loss function
ailayer_softmax_f32_t softmax_layer_2 = AILAYER_SOFTMAX_F32_A();                             // Loss: mean square error / for sigmoid output activation function
ailoss_crossentropy_t crossentropy_loss;

ailayer_dense_f32_t   dense_layer_2   = AILAYER_DENSE_F32_A( /*neurons=*/ num_classes);

void build_AIfES_model() {

  // --------------------------- Define the structure of the model ----------------------------
  // Here the order of the layers is defined.
  // First of all, the input layer needs to be added to the model
  model.input_layer = ailayer_input_f32_default(&input_layer);

  // Then the dense hidden layer is added, it needs the corresponding layer (i.e. &sigmoid_layer_1) and the previous layer (here model.input_layer).
  // It returns the successfully initialized layer structure.
  x = ailayer_dense_f32_default(&dense_layer_1, model.input_layer);
  // Then the activation function of the hidden layer is added to the model.
  // It is implemented as an additional layer, it needs the corresponding layer (i.e. &sigmoid_layer_1) and the previous layer (here x, i.e. sigmoid_layer_1)
  x = ailayer_sigmoid_f32_default(&sigmoid_layer_1, x);

  // Finally, the output layer needs to be added to the model
  // It follow the same semantics as the other layers
  x = ailayer_dense_f32_default(&dense_layer_2, x);

  // When the softmax activation function is not chosen, then the sigmoid activation function is used. The loss is the mean square error

  // Add the softmax activation function to the output layer
  // It is implemented as an additional layer, it needs the corresponding layer (i.e. &sigmoid_layer_2) and the previous layer (here x, i.e. output_layer)
  x = ailayer_softmax_f32_default(&softmax_layer_2, x);

  // Assign the output layer to the model, and therefore also the previous layers (i.e. hidden layer)
  model.output_layer = x;

  // Define the loss function for the training of the ANN
  // It needs the loss model (i.e. &mse_loss) and the output layer of the model (i.e. model.output_layer)
  // model.loss = ailoss_mse_f32_default(&mse_loss, model.output_layer);
  model.loss = ailoss_crossentropy_f32_default(&crossentropy_loss, model.output_layer);

  // Generating the model with the specified structure of the layers
  // Counts the number of layers and trainable parameters in a model as preparation for inference or training.
  aialgo_compile_model(&model);

  // ------------------------------- Allocate memory for the parameters of the model ------------------------------
  // Calculate the memory requirements for the trainable parameters (like weights, bias, ...) of the model.
  uint32_t parameter_memory_size = aialgo_sizeof_parameter_memory(&model);

  // Output of the calculated memory size
  Serial.print(F("Required memory for parameter (Weights, Bias, ...): "));
  Serial.print(parameter_memory_size);
  Serial.println(F("Byte"));

  // Reserve the necessary memory stack
  parameter_memory = malloc(parameter_memory_size);

  // Check if malloc was successful
  if (parameter_memory == NULL) {
    while (true) {
      Serial.println(F("Not enough RAM available. Reduce the number of samples per object and flash the example again."));
      delay(1000);
    }
  }

  // Assign the memory for the trainable parameters (like weights, bias, ...) of the model.
  aialgo_distribute_parameter_memory(&model, parameter_memory, parameter_memory_size);
}


// float train_AIfES_model(EMGData data[48], unsigned int number_data_points) {
float train_AIfES_model(EMGData data[number_data_points]) {
  // In this function the model is trained with the captured training data

  while (!Serial);
  //AIfES requires random weights for training
  //Here the random seed is generated by the noise of an analog pin
  srand(analogRead(A5));
  uint32_t i;

  double biArray[number_data_points];
  double triArray[number_data_points];
  String labelArray[number_data_points];
  // float num_labelArray[number_data_points];
  float num_labelArray[number_data_points][num_classes];
  const int num_rms_values_per_label = 8; 

  for (int i = 0; i < number_data_points; ++i) {
      // Store the first column values (biRMS) in biArray
      biArray[i] = data[i].biRMS;
      triArray[i] = data[i].triRMS;
      labelArray[i] = data[i].label;
  }

  for (int i = 0; i < number_data_points; ++i) {
      for (int j = 0; j < num_classes; ++j) {
          if (i % num_rms_values_per_label == 0) { // Every 8 rows, change the value for the first column
              if (labelArray[i] == "flexion" && j == 0) {
                  num_labelArray[i][j] = 1.0f;
              } else if (labelArray[i] == "extension" && j == 1) {
                  num_labelArray[i][j] = 1.0f;
              } else if (labelArray[i] == "rest" && j == 2) {
                  num_labelArray[i][j] = 1.0f;
              }  else { // rest label:
                  num_labelArray[i][j] = 0.0f;
                  continue;
              }
          } else { // For the rest of the rows, just copy the previous values
              num_labelArray[i][j] = num_labelArray[i - 1][j];
          }
      }
  }

  // // Print the numerical labels in tabular format
  // for (int i = 0; i < number_data_points; ++i) {
  //     Serial.print("Row ");
  //     Serial.print(i);
  //     Serial.print(": ");
  //     for (int j = 0; j < num_classes; ++j) {
  //         Serial.print(num_labelArray[i][j], 2); // Print with 2 decimal places
  //         Serial.print("\t"); // Tab character for spacing
  //     }
  //     Serial.println(); // Move to the next line for the next row
  // }

  // -------------------------------- Create tensors needed for training ---------------------
  uint16_t input_shape[] = {number_data_points, num_features};          
  float input_data[number_data_points*num_features];

  // Populate input_data array with biArray and triArray values
  for (int i = 0; i < number_data_points; ++i) {
      input_data[i * 2] = biArray[i];
      input_data[i * 2 + 1] = triArray[i];
  }

  aitensor_t input_tensor = AITENSOR_2D_F32(input_shape, input_data);  

  uint16_t target_shape[] = {number_data_points, num_classes};  
  float target_data[number_data_points][num_classes];

  for (int i = 0; i < number_data_points; ++i) {
    for (int j = 0; j < num_classes; j++) {
      target_data[i][j] = num_labelArray[i][j];
      // Serial.print(target_data[i][j]);
    }
  } 

  aitensor_t target_tensor = AITENSOR_2D_F32(target_shape, target_data);

  // Define output_data as a 2D array
  float output_data[number_data_points][num_classes];           
  uint16_t output_shape[] = {number_data_points, num_classes};          
  
  aitensor_t output_tensor = AITENSOR_2D_F32(output_shape, output_data);

  // -------------------------------- Initialize the weights and bias of the layers ---------------------
  aimath_f32_default_init_glorot_uniform(&dense_layer_1.weights);
  aimath_f32_default_init_zeros(&dense_layer_1.bias);
  aimath_f32_default_init_glorot_uniform(&dense_layer_2.weights);
  aimath_f32_default_init_zeros(&dense_layer_2.bias);

  // -------------------------------- Define the optimizer for training ---------------------
  // Definition of the pointer towards the optimizer, which helps to optimize the learning process of the ANN
  aiopti_adam_f32_t adam_opti = AIOPTI_ADAM_F32(/*learning rate=*/ 0.001f, /*beta_1=*/ 0.9f, /*beta_2=*/ 0.999f, /*eps=*/ 1e-7);
  aiopti_t *optimizer = aiopti_adam_f32_default(&adam_opti); // Initialize the optimizer

  // -------------------------------- Allocate and schedule the working memory for training ---------
  // Calculate the necessary memory size to store intermediate results, gradients and momentums for training.
  // It needs the model and the optimizer as parameters
  uint32_t memory_size = aialgo_sizeof_training_memory(&model, optimizer);

  // Output of the calculated memory size
  Serial.print(F("Required memory for the training (Intermediate results, gradients, optimization memory): "));
  Serial.print(memory_size);
  Serial.print(F("Byte"));
  Serial.println(F(""));
  
  // Reserve the necessary memory stack
  void *memory_ptr = malloc(memory_size);

  // Check if malloc was successful
  if (memory_ptr == NULL) {
    while (true) {
      Serial.println(F("Not enough RAM available. Reduce the number of samples per Object and flash the example again."));
      delay(1000);
    }
  }

  // Schedule the memory over the model
  aialgo_schedule_training_memory(&model, optimizer, memory_ptr, memory_size);

  // Initialize model for training
  aialgo_init_model_for_training(&model, optimizer);


  // ------------------------------------- Run the training ------------------------------------
  float loss;                                            // Variable to store the loss of the model
  uint32_t batch_size = 1;                  // Setting the batch size, here: full batch
  uint16_t epochs = 100;                                 // Set the number of epochs for training
  uint16_t print_interval = 10;                          // Print every ten epochs the current loss

  Serial.println(F("Start training"));
  for (i = 0; i < epochs; i++) {
    // One epoch of training. Iterates through the whole data once
    aialgo_train_model(&model, &input_tensor, &target_tensor, optimizer, batch_size);

    // Calculate and print loss every print_interval epochs
    if (i % print_interval == 0) {
      // Calculate loss
      aialgo_calc_loss_model_f32(&model, &input_tensor, &target_tensor, &loss);
      // Output current epoch and loss
      Serial.print(F("Epoch: "));
      Serial.print(i);
      Serial.print(F(" Loss: "));
      Serial.print(loss);
      Serial.println(F(""));

    }
  }

  Serial.println(F("Finished training"));

  // ----------------------------------------- Evaluate the trained model --------------------------
  // Here the trained network is tested with the training data. The training data is used as input and the predicted result
  // of the ANN is shown along with the corresponding labels.

  // Run the inference with the trained AIfES model, i.e. predict the output from the training data with the use of the ANN
  // The function needs the trained model, the input_tensor with the input data and the output_tensor where the results are saved in the corresponding array
  aialgo_inference_model(&model, &input_tensor, &output_tensor);

  Serial.println(F(""));
  Serial.println(F("After training:"));
  Serial.println(F("Results:"));
  Serial.println(F("input 1:\tinput 2:\treal output:\tcalculated probabilites:\tpredicted label:"));
  
  uint32_t input_counter = 0;
  String predicted_labels[number_data_points*1];
  int correct_predictions = 0;
  int total_predictions = number_data_points;
  float max_prob;

  // Loop through each data point
  for (int i = 0; i < number_data_points; i++) {
    max_prob = output_data[i][0];
    for (int j = 0; j < num_classes; j++) {
        // if current probability is greater than first probability (flexion):
        if (output_data[i][j] >= max_prob) {
          max_prob = output_data[i][j];
          if (j == 0) { // if num_class index is 0
            predicted_labels[i] = "flexion";
            // check whether predicted label has target value of 1 (indicating label presence). If so, a correct prediction has been made. If not, target value will be equal to 0.
            if (target_data[i][j] == 1) {
              correct_predictions++; 
            }
          }
          else if (j == 1) {
            predicted_labels[i] = "extension";
            if (target_data[i][j] == 1) {
              correct_predictions++; 
            }
          }
          else {
            predicted_labels[i] = "rest";
            if (target_data[i][j] == 1) {
              correct_predictions++; 
            }
          }
        }
    }


    // Print input data
    Serial.print(input_data[input_counter]);
    Serial.print(F("\t\t"));
    input_counter++;
    Serial.print(input_data[input_counter]);
    Serial.print(F("\t\t"));


    for (int j = 0; j < num_classes; j++) {
      Serial.print(target_data[i][j]);
      Serial.print(F(","));
    }

    Serial.print(F("\t"));
    // Print output data for all classes
    for (int j = 0; j < num_classes; j++) {
      Serial.print(output_data[i][j]);
      Serial.print(F(","));
    }

    // Print predicted label
    Serial.print(F("\t\t"));
    Serial.println(predicted_labels[i]);


  }

  float accuracy = (float)correct_predictions / total_predictions * 100;
  Serial.print("Accuracy: ");
  Serial.print(accuracy);
  Serial.println("%");

  return accuracy;
}