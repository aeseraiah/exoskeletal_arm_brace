aimodel_t model;                                    // AIfES model
ailayer_t *x;                                       // Layer object from AIfES, contains the layers

void *parameter_memory = NULL;                      // Pointer to the memory stack of the AIfES model

const int num_classes = 2;

uint16_t input_shape[] = {1, num_classes};   

ailayer_input_f32_t   input_layer     = AILAYER_INPUT_F32_A( /*input dimension=*/ 2, /*input shape=*/ input_shape); 
ailayer_dense_f32_t   dense_layer_1   = AILAYER_DENSE_F32_A( /*neurons=*/ 60); 

ailayer_sigmoid_f32_t sigmoid_layer_1 = AILAYER_SIGMOID_F32_A(); 

// This part is for the sigmoid activation function on the output layer and the mean squared error loss function
ailayer_sigmoid_f32_t sigmoid_layer_2 = AILAYER_SIGMOID_F32_A();                             // Loss: mean square error / for sigmoid output activation function
ailoss_crossentropy_t crossentropy_loss;

ailayer_dense_f32_t   dense_layer_2   = AILAYER_DENSE_F32_A( /*neurons=*/ 1);

void build_AIfES_model() {

  // --------------------------- Define the structure of the model ----------------------------
  // Here the order of the layers is defined.
  // First of all, the input layer needs to be added to the model
  model.input_layer = ailayer_input_f32_default(&input_layer);

  // Then the dense hidden layer is added, it needs the corresponding layer (i.e. &sigmoid_layer_1) and the previous layer (here model.input_layer).
  // It returns the successfully initialized layer structure.
  x = ailayer_dense_f32_default(&dense_layer_1, model.input_layer);
  // Then the activation function of the hidden layer is added to the model.
  // It is implemented as an additional layer, it needs the corresponding layer (i.e. &sigmoid_layer_1) and the previous layer (here x, i.e. sigmoid_layer_1)
  x = ailayer_sigmoid_f32_default(&sigmoid_layer_1, x);

  // Finally, the output layer needs to be added to the model
  // It follow the same semantics as the other layers
  x = ailayer_dense_f32_default(&dense_layer_2, x);

  // When the softmax activation function is not chosen, then the sigmoid activation function is used. The loss is the mean square error

  // Add the sigmoid activation function to the output layer
  // It is implemented as an additional layer, it needs the corresponding layer (i.e. &sigmoid_layer_2) and the previous layer (here x, i.e. output_layer)
  x = ailayer_sigmoid_f32_default(&sigmoid_layer_2, x);

  // Assign the output layer to the model, and therefore also the previous layers (i.e. hidden layer)
  model.output_layer = x;

  // Define the loss function for the training of the ANN
  // It needs the loss model (i.e. &mse_loss) and the output layer of the model (i.e. model.output_layer)
  // model.loss = ailoss_mse_f32_default(&mse_loss, model.output_layer);
  model.loss = ailoss_crossentropy_f32_default(&crossentropy_loss, model.output_layer);

  // Generating the model with the specified structure of the layers
  // Counts the number of layers and trainable parameters in a model as preparation for inference or training.
  aialgo_compile_model(&model);

  // ------------------------------- Allocate memory for the parameters of the model ------------------------------
  // Calculate the memory requirements for the trainable parameters (like weights, bias, ...) of the model.
  uint32_t parameter_memory_size = aialgo_sizeof_parameter_memory(&model);

  // Output of the calculated memory size
  Serial.print(F("Required memory for parameter (Weights, Bias, ...):"));
  Serial.print(parameter_memory_size);
  Serial.println(F("Byte"));

  // Reserve the necessary memory stack
  parameter_memory = malloc(parameter_memory_size);

  // Check if malloc was successful
  if (parameter_memory == NULL) {
    while (true) {
      Serial.println(F("Not enough RAM available. Reduce the number of samples per object and flash the example again."));
      delay(1000);
    }
  }

  // Assign the memory for the trainable parameters (like weights, bias, ...) of the model.
  aialgo_distribute_parameter_memory(&model, parameter_memory, parameter_memory_size);
}


// float train_AIfES_model(EMGData data[32], unsigned int number_data_points) {
float train_AIfES_model(EMGData data[32]) {
  const int number_data_points = 32;
  // In this function the model is trained with the captured training data

  while (!Serial);
  //AIfES requires random weights for training
  //Here the random seed is generated by the noise of an analog pin
  srand(analogRead(A5));
  uint32_t i;

  double biArray[number_data_points];
  double triArray[number_data_points];
  String labelArray[number_data_points];
  float num_labelArray[number_data_points];

  for (int i = 0; i < number_data_points; ++i) {
      // Store the first column values (biRMS) in biArray
      biArray[i] = data[i].biRMS;
      triArray[i] = data[i].triRMS;
      labelArray[i] = data[i].label;
      //Serial.println(labelArray[i]); // check to see if number of labels per second make sense. It should be 4/sec
  }

  for (int i = 0; i < number_data_points; ++i) {
    Serial.println(labelArray[i]);
    if (labelArray[i] == "flexion") {
      num_labelArray[i] = 1.0f;
      // Serial.println(num_labelArray[i]);
    }
    else { 
      num_labelArray[i] = 0.0f;
      // Serial.println(num_labelArray[i]);
    }
  }

  // -------------------------------- Create tensors needed for training ---------------------
  // Create the input tensor for training, contains all samples
  uint16_t input_shape[] = {number_data_points, 2};             // Definition of the shape of the tensor, here: {# of total samples (i.e. samples per object * 3 objects), 3 (i.e. for each sample we have 3 RGB values)}
 
///////// TRAIN WITH REAL-TIME DATA /////////////////////////
  float input_data[number_data_points*2];

  // Populate input_data array with biArray and triArray values
  for (int i = 0; i < number_data_points; ++i) {
      input_data[i * 2] = biArray[i];
      input_data[i * 2 + 1] = triArray[i];
  }

///////// TRAIN WITH OFFLINE DATA /////////////////////////
  // float input_data[number_data_points*2] = {
  //   33.05f,32.89f,
  //   87.00f,77.51f,
  //   94.27f,81.90f,
  //   96.67f,78.30f,
  //   78.28f,61.53f,
  //   95.32f,76.87f,
  //   78.22f,53.66f,
  //   92.20f,75.80f,
  //   15.47f,21.14f,
  //   53.86f,57.77f,
  //   70.11f,75.33f,
  //   66.01f,75.66f,
  //   33.61f,48.81f,
  //   82.46f,85.92f,
  //   49.10f,62.33f,
  //   42.28f,48.70f,
  //   33.05f,32.89f,
  //   87.00f,77.51f,
  //   94.27f,81.90f,
  //   96.67f,78.30f,
  //   78.28f,61.53f,
  //   95.32f,76.87f,
  //   78.22f,53.66f,
  //   92.20f,75.80f,
  //   15.47f,21.14f,
  //   53.86f,57.77f,
  //   70.11f,75.33f,
  //   66.01f,75.66f,
  //   33.61f,48.81f,
  //   82.46f,85.92f,
  //   49.10f,62.33f,
  //   42.28f,48.70f
  // };

  aitensor_t input_tensor = AITENSOR_2D_F32(input_shape, input_data); 
 
  // Create the target tensor for training, contains the desired output for the corresponding sample to train the ANN
  uint16_t target_shape[] = {number_data_points, 1};            // Definition of the shape of the tensor, here: {# of total samples (i.e. samples per object * 3 objects), 3 (i.e. for each sample we have 3 possible output classes)}

////////////////// TRAIN WITH REAL-TIME DATA ////////////////////////
  float target_data[number_data_points*1];

  for (int i = 0; i < number_data_points; ++i) {
      target_data[i] = num_labelArray[i];
  }


////////////////// TRAIN WITH OFFLINE DATA ////////////////////////
  // float target_data[number_data_points*1] = {
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   0.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f,
  //   1.0f
  // };  

  aitensor_t target_tensor = AITENSOR_2D_F32(target_shape, target_data);

  // Create an output tensor for training, here the results of the ANN are saved and compared to the target tensor during training
  float output_data[number_data_points*1];                     // Array for storage of the output data
  uint16_t output_shape[] = {number_data_points, 1};            // Definition of the shape of the tensor, here: {# of total samples (i.e. samples per object * 3 objects), 3 (i.e. for each sample we have 3 possible output classes)}
  
  aitensor_t output_tensor = AITENSOR_2D_F32(output_shape, output_data);

  // -------------------------------- Initialize the weights and bias of the layers ---------------------
  aimath_f32_default_init_glorot_uniform(&dense_layer_1.weights);
  aimath_f32_default_init_zeros(&dense_layer_1.bias);
  aimath_f32_default_init_glorot_uniform(&dense_layer_2.weights);
  aimath_f32_default_init_zeros(&dense_layer_2.bias);

  // -------------------------------- Define the optimizer for training ---------------------
  // Definition of the pointer towards the optimizer, which helps to optimize the learning process of the ANN
  aiopti_adam_f32_t adam_opti = AIOPTI_ADAM_F32(/*learning rate=*/ 0.001f, /*beta_1=*/ 0.9f, /*beta_2=*/ 0.999f, /*eps=*/ 1e-7);
  aiopti_t *optimizer = aiopti_adam_f32_default(&adam_opti); // Initialize the optimizer

  // -------------------------------- Allocate and schedule the working memory for training ---------
  // Calculate the necessary memory size to store intermediate results, gradients and momentums for training.
  // It needs the model and the optimizer as parameters
  uint32_t memory_size = aialgo_sizeof_training_memory(&model, optimizer);

  // Output of the calculated memory size
  Serial.print(F("Required memory for the training (Intermediate results, gradients, optimization memory):"));
  Serial.print(memory_size);
  Serial.print(F("Byte"));
  Serial.println(F(""));
  
  // Reserve the necessary memory stack
  void *memory_ptr = malloc(memory_size);

  // Check if malloc was successful
  if (memory_ptr == NULL) {
    while (true) {
      Serial.println(F("Not enough RAM available. Reduce the number of samples per Object and flash the example again."));
      delay(1000);
    }
  }

  // Schedule the memory over the model
  aialgo_schedule_training_memory(&model, optimizer, memory_ptr, memory_size);

  // Initialize model for training
  aialgo_init_model_for_training(&model, optimizer);


  // ------------------------------------- Run the training ------------------------------------
  float loss;                                            // Variable to store the loss of the model
  uint32_t batch_size = 1;                  // Setting the batch size, here: full batch
  uint16_t epochs = 200;                                 // Set the number of epochs for training
  uint16_t print_interval = 10;                          // Print every ten epochs the current loss

  Serial.println(F("Start training"));
  for (i = 0; i < epochs; i++) {
    // One epoch of training. Iterates through the whole data once
    aialgo_train_model(&model, &input_tensor, &target_tensor, optimizer, batch_size);

    // Calculate and print loss every print_interval epochs
    if (i % print_interval == 0) {
      // Calculate loss
      aialgo_calc_loss_model_f32(&model, &input_tensor, &target_tensor, &loss);
      // Output current epoch and loss
      Serial.print(F("Epoch: "));
      Serial.print(i);
      Serial.print(F(" Loss: "));
      Serial.print(loss);
      Serial.println(F(""));

    }
  }

  Serial.println(F("Finished training"));

  // ----------------------------------------- Evaluate the trained model --------------------------
  // Here the trained network is tested with the training data. The training data is used as input and the predicted result
  // of the ANN is shown along with the corresponding labels.

  // Run the inference with the trained AIfES model, i.e. predict the output from the training data with the use of the ANN
  // The function needs the trained model, the input_tensor with the input data and the output_tensor where the results are saved in the corresponding array
  aialgo_inference_model(&model, &input_tensor, &output_tensor);

  Serial.println(F(""));
  Serial.println(F("After training:"));
  Serial.println(F("Results:"));
  Serial.println(F("input 1:\tinput 2:\treal output:\tcalculated output:\tpredicted label:"));
  
  uint32_t input_counter = 0;
  float predicted_labels[number_data_points*1];
  int correct_predictions = 0;
  int total_predictions = number_data_points;

  
  for (i = 0; i < number_data_points; i++) {
    Serial.print(input_data[input_counter]);    
    //Serial.print(((float* ) input_tensor.data)[i]); //Alternative print for the tensor
    input_counter++;
    Serial.print(F("\t\t"));
    Serial.print(input_data[input_counter]);
    input_counter++;
    Serial.print(F("\t\t"));
    Serial.print(target_data[i]);
    Serial.print(F("\t\t"));
    Serial.print(output_data[i]);
    if (output_data[i] > 0.5) {
      // ACTUATE SERVO:
      // myservo.write(180);
      predicted_labels[i] = 1;
    } 
    else {
      // myservo.write(0);
      predicted_labels[i] = 0;
    }
    Serial.print(F("\t\t\t"));
    Serial.println(predicted_labels[i]);

    if (predicted_labels[i] == target_data[i]) {
      correct_predictions++;
    }

    //Serial.println(((float* ) output_tensor.data)[i]); //Alternative print for the tensor
  }

  float accuracy = (float)correct_predictions / total_predictions * 100;
  Serial.print("Accuracy: ");
  Serial.print(accuracy);
  Serial.println("%");

  return accuracy;
}